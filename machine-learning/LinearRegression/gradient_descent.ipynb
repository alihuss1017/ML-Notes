{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300.0, 500.0])   #target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, w, b):\n",
    "    m = x.shape[0]\n",
    "    cost_sum = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = (f_wb - y[i]) ** 2\n",
    "        cost_sum += cost\n",
    "    total_cost = (1 / (2*m)) * cost_sum\n",
    "    return total_cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is described as: <br> <center> repeat until convergence: {  <br> $$ w = w - \\alpha \\frac {\\partial J(w,b)}{\\partial w}$$  $$ b = b - \\alpha \\frac {\\partial J(w,b)}{\\partial b}$$ } <br> </center> where parameters $ w,b $ are updated simultaneously."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is defined as: <br> $$ \\frac {\\partial J(w,b)}{\\partial w} = \\frac {1}{m} \\displaystyle\\sum_{i=0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} $$\t<br> $$ \\frac {\\partial J(w,b)}{\\partial b} = \\frac {1}{m} \\displaystyle\\sum_{i=0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})$$\t<br>\n",
    "Simultaneously means you calculate the partial derivatives for all parameters before updating any of them.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x,y,w,b):\n",
    "    m = x.shape[0]\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb = w*x[i] + b\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i]\n",
    "        dj_db_i = f_wb - y[i]\n",
    "        dj_dw += dj_dw_i\n",
    "        dj_db += dj_db_i\n",
    "    dj_dw /= m\n",
    "    dj_db /= m\n",
    "\n",
    "    return dj_dw, dj_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x,y,w_in,b_in, alpha, num_iters, cost_function, gradient_function):\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    w = w_in\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        dj_dw, dj_db = compute_gradient(x,y,w,b)\n",
    "        w = w- alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "\n",
    "\n",
    "        if i < 100000:\n",
    "            J_history.append(cost_function(x,y,w,b))\n",
    "            p_history.append([w,b])\n",
    "            if i%math.ceil(num_iters/10) == 0:\n",
    "                print(f'Iteration {i:4}: Cost {J_history[-1]:.2}', f'dj_dw: {dj_dw:.3e}, dj_db: {dj_db: .3e}', f'w: {w:.3e}, b:{b:.5e}')\n",
    "                      \n",
    "    return w, b, J_history, p_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init = 0\n",
    "b_init = 0\n",
    "\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train, y_train, w_init, b_init, tmp_alpha, iterations, compute_cost, compute_gradient)\n",
    "print(f'(w,b) found by gradient descent: {w_final:8.4f}, {b_final:8.4f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
